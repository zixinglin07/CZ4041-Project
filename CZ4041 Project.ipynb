{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13effc6a",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "Process of selecting and transforming training data into new variables that are used as inputs for the model.\n",
    "\n",
    "Domain knowledge, statistics, visualisation\n",
    "Missing values, dirty data, outliers (remove or replace), skew, normalization\n",
    "\n",
    "EDA:\n",
    "Attributes\n",
    "date - Date of the sale data. There are no holiday effects or store closures.\n",
    "store - Store ID\n",
    "item - Item ID\n",
    "sales - Number of items sold at a particular store on a particular date.\n",
    "\n",
    "Entries (Train / Test) : 913000 / 45000\n",
    "Stores (Train / Test) : 1 - 10 / 1 - 10\n",
    "Items (Train / Test) : 1 - 50 / 1 - 50\n",
    "Dates (Train / Test) : 2013-01-01 - 2017-12-31 / 2018-01-01 - 2018-03-31\n",
    "\n",
    "no obvious outliers, data follows seasonal pattern with linear growth\n",
    "\n",
    "\n",
    "Questions to consider:\n",
    "What's the best way to deal with seasonality? \n",
    "Should stores be modeled separately, or can you pool them together? \n",
    "\n",
    "Evaluation metric: SMAPE\n",
    "Facebook Prophet:\n",
    "Pros:\n",
    "\n",
    "Quite easy to use\n",
    "Allows specifying multiple seasonalities\n",
    "Allows specifying special events\n",
    "Can compute quick MAP and slow but accurate Bayesian estimates\n",
    "Provides methods for basic plotting out of the box\n",
    "\n",
    "Cons:\n",
    "\n",
    "Can only treat univariate time series\n",
    "Assumes Gaussian priors\n",
    "Does not provide methods to tune hyper-parameters out of the box (for example, seasonality and trend flexibility priors)\n",
    "\n",
    "TASKS:\n",
    "optuna to optimise hyperparameters\n",
    "tsfresh for feature engineering\n",
    "dtreeviz to visualise xgboost model\n",
    "Implement fbprophet\n",
    "Custom SMAPE eval metric for model.fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8150f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install xgboost\n",
    "!pip install pystan \n",
    "!pip install prophet\n",
    "!pip install featuretools\n",
    "!pip install tsfresh\n",
    "!git clone https://github.com/jeslago/epftoolbox.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de08cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "import featuretools as ft\n",
    "import tsfresh as ts\n",
    "from prophet import Prophet\n",
    "from epftoolbox.evaluation import sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed65ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('epftoolbox')\n",
    "!pip install .\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e43a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a906ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 913000 entries, 0 to 912999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   date    913000 non-null  object\n",
      " 1   store   913000 non-null  int64 \n",
      " 2   item    913000 non-null  int64 \n",
      " 3   sales   913000 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 27.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9ec582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>913000.000000</td>\n",
       "      <td>913000.000000</td>\n",
       "      <td>913000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.500000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>52.250287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.872283</td>\n",
       "      <td>14.430878</td>\n",
       "      <td>28.801144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.500000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>231.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               store           item          sales\n",
       "count  913000.000000  913000.000000  913000.000000\n",
       "mean        5.500000      25.500000      52.250287\n",
       "std         2.872283      14.430878      28.801144\n",
       "min         1.000000       1.000000       0.000000\n",
       "25%         3.000000      13.000000      30.000000\n",
       "50%         5.500000      25.500000      47.000000\n",
       "75%         8.000000      38.000000      70.000000\n",
       "max        10.000000      50.000000     231.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a15370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date'] = pd.to_datetime(df_train['date'], utc=True)\n",
    "#df_train = pd.DatetimeIndex(df_train['date'])\n",
    "\n",
    "df_test['date'] = pd.to_datetime(df_test['date'], utc=True)\n",
    "#df_test = pd.DatetimeIndex(df_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a100717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features\n",
    "df_train['year'] = df_train['date'].dt.year #pd.DatetimeIndex(df_train['ArrivalDate']).year\n",
    "df_train['month'] = df_train['date'].dt.month\n",
    "df_train['dayofyear'] = df_train['date'].dt.dayofyear\n",
    "df_train['dayofweek'] = df_train['date'].dt.dayofweek\n",
    "df_train['quarter'] = df_train['date'].dt.quarter\n",
    "\n",
    "df_test['year'] = df_test['date'].dt.year\n",
    "df_test['month'] = df_test['date'].dt.month\n",
    "df_test['dayofyear'] = df_test['date'].dt.dayofyear\n",
    "df_test['dayofweek'] = df_test['date'].dt.dayofweek\n",
    "df_test['quarter'] = df_test['date'].dt.quarter\n",
    "#df_test.head()\n",
    "\n",
    "# Lag features: using past value of forecast var as a feature\n",
    "df_train.index = pd.DatetimeIndex(df_train['date'])\n",
    "target_map = df_train['sales'].to_dict()\n",
    "df_train['lag_feature'] = (df_train.index - pd.Timedelta('364 days')).map(target_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10bf4803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2017-12-27 00:00:00+00:00    43.0\n",
       "2017-12-28 00:00:00+00:00    68.0\n",
       "2017-12-29 00:00:00+00:00    63.0\n",
       "2017-12-30 00:00:00+00:00    64.0\n",
       "2017-12-31 00:00:00+00:00    69.0\n",
       "Name: lag_feature, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check lag features\n",
    "df_train['lag_feature'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7060476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['store', 'item', 'sales', 'year', 'month', 'dayofyear', 'dayofweek',\n",
       "       'quarter', 'lag_feature'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.drop(['date'], axis=1)\n",
    "df_test = df_test.drop(['date'], axis=1)\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99693c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = ['store', 'item', 'year', 'month', 'dayofweek', 'dayofyear', 'quarter','lag_feature']\n",
    "forecast_var = ['sales']\n",
    "\n",
    "# Do cross validation with 1 year forecast horizon\n",
    "tss = TimeSeriesSplit(n_splits = 5, test_size = 365)\n",
    "df_train = df_train.sort_index()\n",
    "\n",
    "for train_index, validation_index in tss.split(df_train):\n",
    "    df_train_split = df_train.iloc[train_index]\n",
    "    df_validation = df_train.iloc[validation_index]\n",
    "    \n",
    "X_train = df_train_split[features_list]\n",
    "X_validation = df_validation[features_list]\n",
    "y_train = df_train_split[forecast_var]\n",
    "y_validation = df_validation[forecast_var]\n",
    "\n",
    "#X_train, X_validation, y_train, y_validation = train_test_split(df_train[features_list],df_train[forecast_var], train_size=0.8)\n",
    "\n",
    "#X_test = df_test[features_list]\n",
    "#y_test = df_test[forecast_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5612a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:56.78697\n",
      "[10]\tvalidation_0-rmse:56.36713\n",
      "[20]\tvalidation_0-rmse:55.95027\n",
      "[30]\tvalidation_0-rmse:55.52443\n",
      "[40]\tvalidation_0-rmse:55.11365\n",
      "[50]\tvalidation_0-rmse:54.71021\n",
      "[60]\tvalidation_0-rmse:54.31031\n",
      "[70]\tvalidation_0-rmse:53.92092\n",
      "[80]\tvalidation_0-rmse:53.52628\n",
      "[90]\tvalidation_0-rmse:53.13848\n",
      "[100]\tvalidation_0-rmse:52.75935\n",
      "[110]\tvalidation_0-rmse:52.35489\n",
      "[120]\tvalidation_0-rmse:51.91417\n",
      "[130]\tvalidation_0-rmse:51.48708\n",
      "[140]\tvalidation_0-rmse:51.05330\n",
      "[150]\tvalidation_0-rmse:50.63382\n",
      "[160]\tvalidation_0-rmse:50.20793\n",
      "[170]\tvalidation_0-rmse:49.78800\n",
      "[180]\tvalidation_0-rmse:49.36381\n",
      "[190]\tvalidation_0-rmse:48.95615\n",
      "[200]\tvalidation_0-rmse:48.53909\n",
      "[210]\tvalidation_0-rmse:48.14736\n",
      "[220]\tvalidation_0-rmse:47.75231\n",
      "[230]\tvalidation_0-rmse:47.35370\n",
      "[240]\tvalidation_0-rmse:46.97091\n",
      "[250]\tvalidation_0-rmse:46.58177\n",
      "[260]\tvalidation_0-rmse:46.20710\n",
      "[270]\tvalidation_0-rmse:45.83441\n",
      "[280]\tvalidation_0-rmse:45.46320\n",
      "[290]\tvalidation_0-rmse:45.10608\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, early_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m, sample_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, booster \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdart\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# model.load_model('model.json')\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_validation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m (\n\u001b[0;32m   1017\u001b[0m     model,\n\u001b[0;32m   1018\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1024\u001b[0m )\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create model object\n",
    "\"\"\"\n",
    "Parameters to test: learning rate=0.01, max depth=12, reg_lambda=0.5 \n",
    "\n",
    "1. 1000 estimators, learning_rate = 0.001, booster = 'gbtree',rmse:29.75191\n",
    "2. 1000 estimators, learning_rate = 0.001, booster = 'gbtree',rmse:60.14270 w TimeSeriesSplit\n",
    "3. 1000 estimators, learning_rate = 0.001, booster = 'gbtree',rmse:28.29920 w lag feature\n",
    "4. 5000 estimators, learning_rate = 0.001, booster = 'gbtree',rmse:13.17542 w lag feature\n",
    "5. 1000 estimators, learning_rate = 0.001, booster = 'dart',rmse:28.28290 w lag feature. Extremely slow training\n",
    "6. 5000 estimators, learning_rate = 0.001, booster = 'gbtree',rmse:8.90808 w max_depth = 12\n",
    "7. 1000 estimators, learning_rate = 0.01, booster = 'gbtree',rmse:7.87088 w max_depth = 12, starts overfitting with more estimators\n",
    "8. 1000 estimators, learning_rate = 0.01, booster = 'gbtree', max_depth = 12, reg_lambda = 0.5, rmse:7.85207\n",
    "\n",
    "\"\"\"\n",
    "#%%capture output\n",
    "model = xgb.XGBRegressor(n_estimators = 1000, early_stopping_rounds = 50, learning_rate = 0.001, sample_type = 'weighted', booster = 'dart')\n",
    "# model.load_model('model.json')\n",
    "model.fit(X_train, y_train, eval_set = [(X_validation,y_validation)], verbose = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98c2c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['store', 'item', 'year', 'month', 'dayofweek', 'dayofyear', 'quarter', 'lag_feature']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quarter</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.016784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dayofyear</th>\n",
       "      <td>0.017603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dayofweek</th>\n",
       "      <td>0.051125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.099540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store</th>\n",
       "      <td>0.135891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_feature</th>\n",
       "      <td>0.168955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <td>0.510102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Importance\n",
       "quarter        0.000000\n",
       "year           0.016784\n",
       "dayofyear      0.017603\n",
       "dayofweek      0.051125\n",
       "month          0.099540\n",
       "store          0.135891\n",
       "lag_feature    0.168955\n",
       "item           0.510102"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_importance = pd.DataFrame(index = model.get_booster().feature_names, data = model.feature_importances_, columns = ['Importance'])\n",
    "print(model.get_booster().feature_names)\n",
    "df_feature_importance.sort_values('Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "343fe857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sales          0\n",
      "0     13  11.279771\n",
      "1     26  25.576149\n",
      "2     27  20.528736\n",
      "3     54  44.011692\n",
      "4     35  38.287983\n",
      "   sales          0\n",
      "0     35  23.416321\n",
      "1     17  17.925344\n",
      "2     22  22.780067\n",
      "3     21  20.664536\n",
      "4     19  20.180243\n"
     ]
    }
   ],
   "source": [
    "# Slow for complex models\n",
    "y_train_pred = pd.DataFrame(model.predict(X_train))\n",
    "y_validation_pred = pd.DataFrame(model.predict(X_validation))\n",
    "# Convert index from Range64 to Int64 \n",
    "y_train_pred.index = list(y_train_pred.index)\n",
    "y_validation_pred.index = list(y_validation_pred.index)\n",
    "\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_train_pred_real = pd.concat([y_train,y_train_pred],axis=1)\n",
    "y_validation.reset_index(drop=True, inplace=True)\n",
    "y_validation_pred_real = pd.concat([y_validation,y_validation_pred],axis=1)\n",
    "\n",
    "print(y_train_pred_real.head())\n",
    "print(y_validation_pred_real.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d5b4f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set sMAPE:  0.11763192366525083\n",
      "Validation set sMAPE:  0.13544394278689448\n"
     ]
    }
   ],
   "source": [
    "# custom evaluation metric, target SMAPE <0.02\n",
    "\"\"\"\n",
    "1. Train set sMAPE:  0.43540993893878704 Validation set sMAPE:  0.4353321748664415\n",
    "2. Train set sMAPE:  0.43428628486970966 Validation set sMAPE:  0.8993101261777344\n",
    "3. Train set sMAPE:  0.43886955493496904 Validation set sMAPE:  0.4591814227399213\n",
    "4. Train set sMAPE:  0.21175586516844072 Validation set sMAPE:  0.2135402045304419\n",
    "5. None\n",
    "6. Train set sMAPE:  0.13079433706697108 Validation set sMAPE:  0.15268113263167146\n",
    "7. Train set sMAPE:  0.11717552685158845 Validation set sMAPE:  0.13378531448480144\n",
    "\"\"\"\n",
    "train_sMAPE = sMAPE(p_pred=y_train_pred, p_real=y_train) \n",
    "validation_sMAPE = sMAPE(p_pred=y_validation_pred, p_real=y_validation) \n",
    "print(\"Train set sMAPE: \", train_sMAPE)\n",
    "print(\"Validation set sMAPE: \", validation_sMAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96766c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_model('model_9.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876aea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to predict on test data\n",
    "df_test['sales_predicted'] = model.predict(X_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900430d1",
   "metadata": {},
   "source": [
    "There are in general two ways that you can control overfitting in XGBoost:<br>\n",
    "\n",
    "The first way is to directly control model complexity.<br>\n",
    "\n",
    "This includes max_depth, min_child_weight and gamma.<br>\n",
    "\n",
    "The second way is to add randomness to make training robust to noise.<br>\n",
    "\n",
    "This includes subsample and colsample_bytree.<br>\n",
    "\n",
    "You can also reduce stepsize eta. Remember to increase num_round when you do so.\n",
    "<br><br>\n",
    "There’s a parameter called tree_method, set it to hist or gpu_hist for faster computation.\n",
    "<br><br>\n",
    "gbtree parameters:\n",
    "<br>\n",
    "eta [default=0.3, alias: learning_rate]<br>\n",
    "\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.<br>\n",
    "max_depth [default=6]<br>\n",
    "\n",
    "Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree.<br>\n",
    "lambda [default=1, alias: reg_lambda]<br>\n",
    "\n",
    "L2 regularization term on weights. Increasing this value will make model more conservative.<br>\n",
    "\n",
    "The booster dart inherits gbtree booster, so it supports all parameters that gbtree does, such as eta, gamma, max_depth etc.\n",
    "<br><br>\n",
    "Additional parameters are noted below:<br>\n",
    "\n",
    "sample_type: type of sampling algorithm.<br>\n",
    "\n",
    "uniform: (default) dropped trees are selected uniformly.<br>\n",
    "\n",
    "weighted: dropped trees are selected in proportion to weight.<br>\n",
    "\n",
    "normalize_type: type of normalization algorithm.<br>\n",
    "\n",
    "tree: (default) New trees have the same weight of each of dropped trees.<br>\n",
    "forest: New trees have the same weight of sum of dropped trees (forest).<br>\n",
    "rate_drop: dropout rate.<br>\n",
    "\n",
    "range: [0.0, 1.0]<br>\n",
    "\n",
    "skip_drop: probability of skipping dropout.<br>\n",
    "\n",
    "If a dropout is skipped, new trees are added in the same manner as gbtree.<br>\n",
    "\n",
    "range: [0.0, 1.0]<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc6b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
